{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, absolute_import\n",
    "import argparse\n",
    "import os.path as osp\n",
    "\n",
    "import numpy as np\n",
    "import sys\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.backends import cudnn\n",
    "from torch.utils.data import DataLoader\n",
    "import reid\n",
    "from reid import datasets\n",
    "from reid import models\n",
    "from reid.dist_metric import DistanceMetric\n",
    "from reid.trainers import Trainer\n",
    "from reid.evaluators import Evaluator\n",
    "from reid.utils.data import transforms as T\n",
    "from reid.utils.data.preprocessor import Preprocessor\n",
    "from reid.utils.logging import Logger\n",
    "from reid.utils.serialization import load_checkpoint, save_checkpoint\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(name, split_id, data_dir, height, width, batch_size, workers,\n",
    "             combine_trainval):\n",
    "    root = osp.join(data_dir, name)\n",
    "\n",
    "    dataset = datasets.create(name, root, split_id=split_id)\n",
    "\n",
    "    normalizer = T.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                             std=[0.229, 0.224, 0.225])\n",
    "\n",
    "    train_set = dataset.trainval if combine_trainval else dataset.train\n",
    "    num_classes = (dataset.num_trainval_ids if combine_trainval\n",
    "                   else dataset.num_train_ids)\n",
    "\n",
    "    train_transformer = T.Compose([\n",
    "        T.RandomSizedRectCrop(height, width),\n",
    "        T.RandomHorizontalFlip(),\n",
    "        T.ToTensor(),\n",
    "        normalizer,\n",
    "    ])\n",
    "\n",
    "    test_transformer = T.Compose([\n",
    "        T.RectScale(height, width),\n",
    "        T.ToTensor(),\n",
    "        normalizer,\n",
    "    ])\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        #images_dir = osp.join(self.root, 'images')\n",
    "        Preprocessor(train_set, root=dataset.images_dir,\n",
    "                     transform=train_transformer),\n",
    "        batch_size=batch_size, num_workers=workers,\n",
    "        shuffle=True, pin_memory=True, drop_last=True)\n",
    "\n",
    "    train_loader_head = DataLoader(\n",
    "        Preprocessor(train_set, root=\"data/viper/images_head\",\n",
    "                     transform=train_transformer),\n",
    "        batch_size=batch_size, num_workers=workers,\n",
    "        shuffle=True, pin_memory=True, drop_last=True)\n",
    "    train_loader_upper = DataLoader(\n",
    "        Preprocessor(train_set, root=\"data/viper/images_upper\",\n",
    "                     transform=train_transformer),\n",
    "        batch_size=batch_size, num_workers=workers,\n",
    "        shuffle=True, pin_memory=True, drop_last=True)\n",
    "    train_loader_lower = DataLoader(\n",
    "        Preprocessor(train_set, root=\"data/viper/images_lower\",\n",
    "                     transform=train_transformer),\n",
    "        batch_size=batch_size, num_workers=workers,\n",
    "        shuffle=True, pin_memory=True, drop_last=True)\n",
    "\n",
    "    val_loader = DataLoader(\n",
    "        Preprocessor(dataset.val, root=dataset.images_dir,\n",
    "                     transform=test_transformer),\n",
    "        batch_size=batch_size, num_workers=workers,\n",
    "        shuffle=False, pin_memory=True)\n",
    "    val_loader_head = DataLoader(\n",
    "        Preprocessor(dataset.val, root=\"data/viper/images_head\",\n",
    "                     transform=test_transformer),\n",
    "        batch_size=batch_size, num_workers=workers,\n",
    "        shuffle=False, pin_memory=True)\n",
    "    val_loader_upper = DataLoader(\n",
    "        Preprocessor(dataset.val, root=\"data/viper/images_upper\",\n",
    "                     transform=test_transformer),\n",
    "        batch_size=batch_size, num_workers=workers,\n",
    "        shuffle=False, pin_memory=True)\n",
    "    val_loader_lower = DataLoader(\n",
    "        Preprocessor(dataset.val, root=\"data/viper/images_lower\",\n",
    "                     transform=test_transformer),\n",
    "        batch_size=batch_size, num_workers=workers,\n",
    "        shuffle=False, pin_memory=True)\n",
    "\n",
    "    test_loader = DataLoader(\n",
    "        Preprocessor(list(set(dataset.query) | set(dataset.gallery)),\n",
    "                     root=dataset.images_dir, transform=test_transformer),\n",
    "        batch_size=batch_size, num_workers=workers,\n",
    "        shuffle=False, pin_memory=True)\n",
    "\n",
    "    test_loader_head = DataLoader(\n",
    "        Preprocessor(list(set(dataset.query) | set(dataset.gallery)),\n",
    "                     root=\"data/viper/images_head\", transform=test_transformer),\n",
    "        batch_size=batch_size, num_workers=workers,\n",
    "        shuffle=False, pin_memory=True)\n",
    "    test_loader_upper = DataLoader(\n",
    "        Preprocessor(list(set(dataset.query) | set(dataset.gallery)),\n",
    "                     root=\"data/viper/images_upper\", transform=test_transformer),\n",
    "        batch_size=batch_size, num_workers=workers,\n",
    "        shuffle=False, pin_memory=True)\n",
    "    test_loader_lower = DataLoader(\n",
    "        Preprocessor(list(set(dataset.query) | set(dataset.gallery)),\n",
    "                     root=\"data/viper/images_lower\", transform=test_transformer),\n",
    "        batch_size=batch_size, num_workers=workers,\n",
    "        shuffle=False, pin_memory=True)\n",
    "\n",
    "\n",
    "\n",
    "    return dataset, num_classes, train_loader, train_loader_head, train_loader_upper, train_loader_lower,\\\n",
    "    val_loader, val_loader_head, val_loader_upper, val_loader_lower, test_loader, test_loader_head, \\\n",
    "    test_loader_upper, test_loader_lower\n",
    "\n",
    "\n",
    "def main(args):\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    cudnn.benchmark = True\n",
    "\n",
    "    # Redirect print to both console and log file\n",
    "    if not args.evaluate:\n",
    "        sys.stdout = Logger(osp.join(args.logs_dir, 'log.txt'))\n",
    "\n",
    "    # Create data loaders\n",
    "    if args.height is None or args.width is None:\n",
    "        args.height, args.width = (144, 56) if args.arch == 'inception' else \\\n",
    "                                  (256, 128)\n",
    "    dataset, num_classes, train_loader, train_loader_head, train_loader_upper, train_loader_lower,\\\n",
    "    val_loader, val_loader_head, val_loader_upper, val_loader_lower,\\\n",
    "    test_loader, test_loader_head, test_loader_upper, test_loader_lower= \\\n",
    "        get_data(args.dataset, args.split, args.data_dir, args.height,\n",
    "                 args.width, args.batch_size, args.workers,\n",
    "                 args.combine_trainval)\n",
    "    # create model1 model2 model3  然后修改optimizer?\n",
    "    # Create model\n",
    "    model = models.create(args.arch, num_features=args.features,\n",
    "                          dropout=args.dropout, num_classes=num_classes)   \n",
    "    model_head = models.create(args.arch, num_features=args.features,\n",
    "                          dropout=args.dropout, num_classes=num_classes)\n",
    "    model_upper = models.create(args.arch, num_features=args.features,\n",
    "                          dropout=args.dropout, num_classes=num_classes)\n",
    "    model_lower = models.create(args.arch, num_features=args.features,\n",
    "                          dropout=args.dropout, num_classes=num_classes)\n",
    "    \n",
    "    # Load from checkpoint\n",
    "    start_epoch = best_top1 = 0\n",
    "#    if args.resume:\n",
    "#       checkpoint = load_checkpoint(args.resume)\n",
    "#        model.load_state_dict(checkpoint['state_dict'])\n",
    "#        start_epoch = checkpoint['epoch']\n",
    "#        best_top1 = checkpoint['best_top1']\n",
    "#        print(\"=> Start epoch {}  best top1 {:.1%}\"\n",
    "#              .format(start_epoch, best_top1))\n",
    "    model = nn.DataParallel(model).cuda()\n",
    "    model_head = nn.DataParallel(model_head).cuda()\n",
    "    model_upper = nn.DataParallel(model_upper).cuda()\n",
    "    model_lower = nn.DataParallel(model_lower).cuda()\n",
    "\n",
    "\n",
    "    # Distance metric\n",
    "    metric = DistanceMetric(algorithm=args.dist_metric)\n",
    "\n",
    "    # Evaluator\n",
    "    evaluator = Evaluator(model, model_head, model_upper, model_lower)\n",
    "\n",
    "#    if args.evaluate:\n",
    "#        metric.train(model, train_loader)\n",
    "#        print(\"Validation:\")\n",
    "#        evaluator.evaluate(val_loader, dataset.val, dataset.val, metric)\n",
    "#        print(\"Test:\")\n",
    "#        evaluator.evaluate(test_loader, dataset.query, dataset.gallery, metric)\n",
    "#        return\n",
    "\n",
    "    # Criterion\n",
    "    criterion = nn.CrossEntropyLoss().cuda()\n",
    "\n",
    "    # Optimizer\n",
    "    if hasattr(model.module, 'base'):\n",
    "        base_param_ids = set(map(id, model.module.base.parameters()))\n",
    "        new_params = [p for p in model.parameters() if\n",
    "                      id(p) not in base_param_ids]\n",
    "        param_groups = [\n",
    "            {'params': model.module.base.parameters(), 'lr_mult': 0.1},\n",
    "            {'params': new_params, 'lr_mult': 1.0}]\n",
    "    else:\n",
    "        param_groups = model.parameters()\n",
    "    optimizer = torch.optim.SGD(param_groups, lr=args.lr,\n",
    "                                momentum=args.momentum,\n",
    "                                weight_decay=args.weight_decay,\n",
    "                                nesterov=True)\n",
    "\n",
    "    optimizer_head = optimizer \n",
    "    optimizer_upper = optimizer \n",
    "    optimizer_lower = optimizer \n",
    "\n",
    "    # Trainer\n",
    "    trainer = Trainer(model, criterion)\n",
    "    trainer_head = Trainer(model_head, criterion)\n",
    "    trainer_upper = Trainer(model_upper, criterion)\n",
    "    trainer_lower = Trainer(model_lower, criterion)\n",
    "\n",
    "    # Schedule learning rate\n",
    "    def adjust_lr(epoch):\n",
    "        step_size = 60 if args.arch == 'inception' else 40\n",
    "        lr = args.lr * (0.1 ** (epoch // step_size))\n",
    "        for g in optimizer.param_groups:\n",
    "            g['lr'] = lr * g.get('lr_mult', 1)\n",
    "\n",
    "    # Start training\n",
    "    for epoch in range(start_epoch, args.epochs):\n",
    "        adjust_lr(epoch)\n",
    "        trainer.train(epoch, train_loader, optimizer)\n",
    "        trainer_head.train(epoch, train_loader_head, optimizer_head)\n",
    "        trainer_upper.train(epoch, train_loader_upper, optimizer_upper)\n",
    "        trainer_lower.train(epoch, train_loader_lower, optimizer_lower)\n",
    "        if epoch < args.start_save:\n",
    "            continue\n",
    "        top1 = evaluator.evaluate(val_loader,val_loader_head, val_loader_upper, val_loader_lower, dataset.val, dataset.val)\n",
    "\n",
    "        is_best = top1 > best_top1\n",
    "        best_top1 = max(top1, best_top1)\n",
    "        save_checkpoint({\n",
    "            'state_dict': model.module.state_dict(),\n",
    "            'epoch': epoch + 1,\n",
    "            'best_top1': best_top1,\n",
    "        }, is_best, fpath=osp.join(args.logs_dir, 'checkpoint.pth.tar'), opath=osp.join(args.logs_dir, 'model_best.pth.tar'))\n",
    "\n",
    "        save_checkpoint_head({\n",
    "            'state_dict': model_head.module.state_dict(),\n",
    "            'epoch': epoch + 1,\n",
    "            'best_top1': best_top1,\n",
    "        }, is_best, fpath=osp.join(args.logs_dir, 'checkpoint_head.pth.tar'), opath=osp.join(args.logs_dir, 'model_head_best.pth.tar'))\n",
    "\n",
    "        save_checkpoint({\n",
    "            'state_dict': model_upper.module.state_dict(),\n",
    "            'epoch': epoch + 1,\n",
    "            'best_top1': best_top1,\n",
    "        }, is_best, fpath=osp.join(args.logs_dir, 'checkpoint_upper.pth.tar'), opath=osp.join(args.logs_dir, 'model_upper_best.pth.tar'))\n",
    "\n",
    "        save_checkpoint({\n",
    "            'state_dict': model_lower.module.state_dict(),\n",
    "            'epoch': epoch + 1,\n",
    "            'best_top1': best_top1,\n",
    "        }, is_best, fpath=osp.join(args.logs_dir, 'checkpoint_lower.pth.tar'), opath=osp.join(args.logs_dir, 'model_lower_best.pth.tar'))\n",
    "\n",
    "\n",
    "        print('\\n * Finished epoch {:3d}  top1: {:5.1%}  best: {:5.1%}{}\\n'.\n",
    "              format(epoch, top1, best_top1, ' *' if is_best else ''))\n",
    "\n",
    "    # Final test\n",
    "    print('Test with best model:')\n",
    "    checkpoint = load_checkpoint(osp.join(args.logs_dir, 'model_best.pth.tar'))\n",
    "    checkpoint_head = load_checkpoint(osp.join(args.logs_dir, 'model_head_best.pth.tar'))\n",
    "    checkpoint_upper = load_checkpoint(osp.join(args.logs_dir, 'model_upper_best.pth.tar'))\n",
    "    checkpoint_lower = load_checkpoint(osp.join(args.logs_dir, 'model__lower_best.pth.tar'))\n",
    "    model.module.load_state_dict(checkpoint['state_dict'])\n",
    "    model_head.module.load_state_dict(checkpoint_head['state_dict'])\n",
    "    model_upper.module.load_state_dict(checkpoint_upper['state_dict'])\n",
    "    model_lower.module.load_state_dict(checkpoint_lower['state_dict'])\n",
    "    metric.train(model, train_loader)\n",
    "    metric.train(model_head, train_loader_head)\n",
    "    metric.train(model_upper, train_loader_upper)\n",
    "    metric.train(model_lower, train_loader_lower)\n",
    "\n",
    "    evaluator.evaluate(test_loader, test_loader_head, test_loader_upper, test_loader_lower, dataset.query, dataset.gallery, metric)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser(description=\"Softmax loss classification\")\n",
    "    # data\n",
    "    parser.add_argument('-d', '--dataset', type=str, default='cuhk03',\n",
    "                        choices=datasets.names())\n",
    "    parser.add_argument('-b', '--batch-size', type=int, default=256)\n",
    "    parser.add_argument('-j', '--workers', type=int, default=4)\n",
    "    parser.add_argument('--split', type=int, default=0)\n",
    "    parser.add_argument('--height', type=int,\n",
    "                        help=\"input height, default: 256 for resnet*, \"\n",
    "                             \"144 for inception\")\n",
    "    parser.add_argument('--width', type=int,\n",
    "                        help=\"input width, default: 128 for resnet*, \"\n",
    "                             \"56 for inception\")\n",
    "    parser.add_argument('--combine-trainval', action='store_true',\n",
    "                        help=\"train and val sets together for training, \"\n",
    "                             \"val set alone for validation\")\n",
    "    # model\n",
    "    parser.add_argument('-a', '--arch', type=str, default='resnet50',\n",
    "                        choices=models.names())\n",
    "    parser.add_argument('--features', type=int, default=128)\n",
    "    parser.add_argument('--dropout', type=float, default=0.5)\n",
    "    # optimizer\n",
    "    parser.add_argument('--lr', type=float, default=0.1,\n",
    "                        help=\"learning rate of new parameters, for pretrained \"\n",
    "                             \"parameters it is 10 times smaller than this\")\n",
    "    parser.add_argument('--momentum', type=float, default=0.9)\n",
    "    parser.add_argument('--weight-decay', type=float, default=5e-4)\n",
    "    # training configs\n",
    "    parser.add_argument('--resume', type=str, default='', metavar='PATH')\n",
    "    parser.add_argument('--evaluate', action='store_true',\n",
    "                        help=\"evaluation only\")\n",
    "    parser.add_argument('--epochs', type=int, default=50)\n",
    "    parser.add_argument('--start_save', type=int, default=0,\n",
    "                        help=\"start saving checkpoints after specific epoch\")\n",
    "    parser.add_argument('--seed', type=int, default=1)\n",
    "    parser.add_argument('--print-freq', type=int, default=1)\n",
    "    # metric learning\n",
    "    parser.add_argument('--dist-metric', type=str, default='euclidean',\n",
    "                        choices=['euclidean', 'kissme'])\n",
    "    # misc\n",
    "    working_dir = osp.dirname(osp.abspath(__file__))\n",
    "    parser.add_argument('--data-dir', type=str, metavar='PATH',\n",
    "                        default=osp.join(working_dir, 'data'))\n",
    "    parser.add_argument('--logs-dir', type=str, metavar='PATH',\n",
    "                        default=osp.join(working_dir, 'logs'))\n",
    "    main(parser.parse_args())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch]",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
